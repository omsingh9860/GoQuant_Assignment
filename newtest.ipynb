{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe39263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data handling, modeling, and performance measurement\n",
    "import os, gc, warnings, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Hide warning messages to keep the output clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Show more columns when printing dataframes\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# Set your base working folder — where 'train' and 'test' folders are located\n",
    "BASE_DIR = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\"\n",
    "TRAIN_DIR = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\"\n",
    "TEST_DIR  = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\test\"\n",
    "\n",
    "# List of cryptocurrencies we are working with\n",
    "ASSETS = [\"ETH\", \"BTC\", \"DOGE\", \"DOT\", \"LINK\", \"SHIB\", \"SOL\"]\n",
    "\n",
    "# Define the numeric type we'll use for better performance and lower memory usage\n",
    "DTYPE_FLOAT = np.float32\n",
    "\n",
    "def parse_ts(s: pd.Series) -> pd.Series:\n",
    "    # First try to convert to datetime using the standard format (e.g. \"2023-08-17 14:30:00\")\n",
    "    ts = pd.to_datetime(s, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\", utc=False)\n",
    "    \n",
    "    # If more than 1% of dates failed to parse, try another common format (e.g. \"17-08-2023 14:30\")\n",
    "    if ts.isna().mean() > 0.01:\n",
    "        ts2 = pd.to_datetime(s, format=\"%d-%m-%Y %H:%M\", errors=\"coerce\", utc=False)\n",
    "        \n",
    "        # If the second format works better, use it\n",
    "        if ts2.isna().mean() < ts.isna().mean():\n",
    "            ts = ts2\n",
    "        else:\n",
    "            # If both specific formats fail, fall back to the generic parser\n",
    "            ts = pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
    "    \n",
    "    return ts\n",
    "\n",
    "def safe_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert all columns (except timestamp) to numeric type\n",
    "    # If something can't be converted, set it to NaN\n",
    "    for c in df.columns:\n",
    "        if c == \"timestamp\":\n",
    "            continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(DTYPE_FLOAT)\n",
    "    return df\n",
    "\n",
    "\n",
    "def winsorize_df(df: pd.DataFrame, lower=0.01, upper=0.99) -> pd.DataFrame:\n",
    "    # Limit extreme values in each numeric column based on percentiles\n",
    "    # Helps reduce the impact of outliers\n",
    "    for c in df.columns:\n",
    "        if c == \"timestamp\":\n",
    "            continue\n",
    "        s = df[c]\n",
    "        ql, qu = s.quantile(lower), s.quantile(upper)\n",
    "        df[c] = s.clip(lower=ql, upper=qu)\n",
    "    return df\n",
    "\n",
    "def ffill_bfill_min(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Fill missing values by using previous known value (forward fill)\n",
    "    # and then the next known value if needed (backward fill)\n",
    "    return df.sort_values(\"timestamp\").ffill().bfill(limit=1)\n",
    "\n",
    "def pearson_corr(y_true, y_pred) -> float:\n",
    "    # Calculate how strongly the true and predicted values are related (Pearson correlation)\n",
    "    yt = np.asarray(y_true, dtype=np.float64)\n",
    "    yp = np.asarray(y_pred, dtype=np.float64)\n",
    "    yt = yt - yt.mean()\n",
    "    yp = yp - yp.mean()\n",
    "    denom = (np.sqrt((yt**2).sum()) * np.sqrt((yp**2).sum()))\n",
    "    \n",
    "    # Return the correlation, or 0 if something went wrong\n",
    "    return float((yt*yp).sum() / denom) if denom > 0 else 0.0\n",
    "\n",
    "\n",
    "def build_eth_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build detailed features for ETH using order book data and price statistics.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid changing the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert timestamps to datetime format and clean any bad or duplicate rows\n",
    "    df[\"timestamp\"] = parse_ts(df[\"timestamp\"])\n",
    "    df = df.dropna(subset=[\"timestamp\"]).drop_duplicates(subset=[\"timestamp\"])\n",
    "    \n",
    "    # Convert all non-timestamp columns to numbers and sort by time\n",
    "    df = safe_numeric(df).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # === Feature 1: Best level spread (Level 1 spread between ask and bid prices)\n",
    "    df[\"spread_l1\"] = (df[\"ask_price1\"] - df[\"bid_price1\"]).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 2: Total volume in the top 5 bid and ask levels\n",
    "    bid_vol_cols = [f\"bid_volume{i}\" for i in range(1, 6) if f\"bid_volume{i}\" in df.columns]\n",
    "    ask_vol_cols = [f\"ask_volume{i}\" for i in range(1, 6) if f\"ask_volume{i}\" in df.columns]\n",
    "    \n",
    "    df[\"depth_bid_5\"] = df[bid_vol_cols].sum(axis=1).astype(DTYPE_FLOAT)\n",
    "    df[\"depth_ask_5\"] = df[ask_vol_cols].sum(axis=1).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 3: Order book imbalance (more bids or asks?)\n",
    "    df[\"imbalance_5\"] = (\n",
    "        (df[\"depth_bid_5\"] - df[\"depth_ask_5\"]) / (df[\"depth_bid_5\"] + df[\"depth_ask_5\"] + 1e-6)\n",
    "    ).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 4: Microprice (weighted average of bid/ask using volume)\n",
    "    denom = (df[\"bid_volume1\"] + df[\"ask_volume1\"]).replace(0, np.nan)\n",
    "    df[\"microprice\"] = (\n",
    "        (df[\"ask_price1\"] * df[\"bid_volume1\"] + df[\"bid_price1\"] * df[\"ask_volume1\"]) / denom\n",
    "    ).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 5: Microprice spread (difference between microprice and mid price)\n",
    "    df[\"micro_spread\"] = (df[\"microprice\"] - df[\"mid_price\"]).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 6: Log returns (1-second)\n",
    "    df[\"ret_1s\"] = np.log(df[\"mid_price\"]).diff().astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 7: Realized volatility over short time windows\n",
    "    for w in [5, 10, 30, 60]:\n",
    "        df[f\"rv_{w}s\"] = (\n",
    "            df[\"ret_1s\"].rolling(w).apply(lambda x: np.sqrt(np.sum(x**2)), raw=True)\n",
    "        ).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Feature 8: Lags for temporal patterns (ret, imbalance, spread)\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        df[f\"ret_1s_lag{lag}\"] = df[\"ret_1s\"].shift(lag).astype(DTYPE_FLOAT)\n",
    "        df[f\"imbalance_5_lag{lag}\"] = df[\"imbalance_5\"].shift(lag).astype(DTYPE_FLOAT)\n",
    "        df[f\"spread_l1_lag{lag}\"] = df[\"spread_l1\"].shift(lag).astype(DTYPE_FLOAT)\n",
    "\n",
    "    # === Final feature selection (a compact but informative subset)\n",
    "    keep = [\"timestamp\", \"label\", \"mid_price\", \"spread_l1\", \"depth_bid_5\", \"depth_ask_5\", \"imbalance_5\",\n",
    "            \"microprice\", \"micro_spread\", \"ret_1s\"] \\\n",
    "           + [f\"rv_{w}s\" for w in [5, 10, 30, 60]] \\\n",
    "           + [f\"ret_1s_lag{l}\" for l in [1, 2, 3, 5, 10]] \\\n",
    "           + [f\"imbalance_5_lag{l}\" for l in [1, 2, 3, 5, 10]] \\\n",
    "           + [f\"spread_l1_lag{l}\" for l in [1, 2, 3, 5, 10]]\n",
    "\n",
    "    # Only keep columns that are actually present in the DataFrame\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    out = df[keep].copy()\n",
    "\n",
    "    # === Clean the final features:\n",
    "    # 1. Clip extreme values (winsorize)\n",
    "    # 2. Fill missing values (forward + backward fill)\n",
    "    # 3. Replace any leftover infs with 0\n",
    "    out = winsorize_df(out, 0.01, 0.99)\n",
    "    out = ffill_bfill_min(out)\n",
    "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0).astype({c: DTYPE_FLOAT for c in out.columns if c != \"timestamp\"})\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb4c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETH feature shape: (631292, 29)\n",
      "            timestamp     label    mid_price  spread_l1  depth_bid_5  \\\n",
      "0 2024-09-25 18:13:28  0.000060  2581.604980    0.01001  3361.200195   \n",
      "1 2024-09-25 18:13:29  0.000057  2581.284912    0.01001  1533.900024   \n",
      "2 2024-09-25 18:13:30  0.000080  2581.284912    0.01001   957.200012   \n",
      "3 2024-09-25 18:13:31  0.000087  2581.104980    0.01001  2510.899902   \n",
      "4 2024-09-25 18:13:32  0.000090  2581.104980    0.01001  2526.399902   \n",
      "\n",
      "   depth_ask_5  imbalance_5   microprice  micro_spread    ret_1s     rv_5s  \\\n",
      "0   373.000031     0.800225  2581.609131      0.004150 -0.000124  0.000000   \n",
      "1  1201.699951     0.121436  2581.286377      0.001465 -0.000124  0.000000   \n",
      "2  1186.900024    -0.107131  2581.285156      0.000244  0.000000  0.000000   \n",
      "3  1292.599976     0.320310  2581.107178      0.002197 -0.000070  0.000000   \n",
      "4  1222.099976     0.347952  2581.107178      0.002197  0.000000  0.000157   \n",
      "\n",
      "   rv_10s  rv_30s  rv_60s  ret_1s_lag1  ret_1s_lag2  ret_1s_lag3  ret_1s_lag5  \\\n",
      "0     0.0     0.0     0.0     0.000000     0.000000     0.000000          0.0   \n",
      "1     0.0     0.0     0.0    -0.000124     0.000000     0.000000          0.0   \n",
      "2     0.0     0.0     0.0    -0.000124    -0.000124     0.000000          0.0   \n",
      "3     0.0     0.0     0.0     0.000000    -0.000124    -0.000124          0.0   \n",
      "4     0.0     0.0     0.0    -0.000070     0.000000    -0.000124          0.0   \n",
      "\n",
      "   ret_1s_lag10  imbalance_5_lag1  imbalance_5_lag2  imbalance_5_lag3  \\\n",
      "0           0.0          0.800225          0.000000          0.000000   \n",
      "1           0.0          0.800225          0.800225          0.000000   \n",
      "2           0.0          0.121436          0.800225          0.800225   \n",
      "3           0.0         -0.107131          0.121436          0.800225   \n",
      "4           0.0          0.320310         -0.107131          0.121436   \n",
      "\n",
      "   imbalance_5_lag5  imbalance_5_lag10  spread_l1_lag1  spread_l1_lag2  \\\n",
      "0          0.000000                0.0         0.01001         0.00000   \n",
      "1          0.000000                0.0         0.01001         0.01001   \n",
      "2          0.000000                0.0         0.01001         0.01001   \n",
      "3          0.000000                0.0         0.01001         0.01001   \n",
      "4          0.800225                0.0         0.01001         0.01001   \n",
      "\n",
      "   spread_l1_lag3  spread_l1_lag5  spread_l1_lag10  \n",
      "0         0.00000         0.00000              0.0  \n",
      "1         0.00000         0.00000              0.0  \n",
      "2         0.01001         0.00000              0.0  \n",
      "3         0.01001         0.00000              0.0  \n",
      "4         0.01001         0.01001              0.0  \n",
      "Merged BTC: +5 cols → shape=(631292, 34); NaN%=0.0000\n",
      "Merged DOGE: +5 cols → shape=(631292, 39); NaN%=0.0000\n",
      "Merged DOT: +5 cols → shape=(631292, 44); NaN%=0.0000\n",
      "Merged LINK: +5 cols → shape=(631292, 49); NaN%=0.0000\n",
      "Merged SHIB: +5 cols → shape=(631292, 54); NaN%=0.0000\n",
      "Merged SOL: +5 cols → shape=(631292, 59); NaN%=0.0000\n",
      "Final train X shape: (631292, 57)  y shape: (631292,)\n"
     ]
    }
   ],
   "source": [
    "# ==== Load ETH training data and create features ====\n",
    "eth_train = pd.read_csv(TRAIN_DIR + \"/ETH.csv\")  # Read ETH CSV file into a dataframe\n",
    "eth_feat = build_eth_features(eth_train)        # Process and create detailed features for ETH\n",
    "\n",
    "print(\"ETH feature shape:\", eth_feat.shape)     # Show the size of the new ETH feature dataframe\n",
    "print(eth_feat.head())                           # Display the first few rows to check data looks right\n",
    "\n",
    "# ==== Add features from other crypto assets onto ETH data ====\n",
    "Xy = eth_feat.copy()  # Start with ETH features as base\n",
    "\n",
    "for asset in [\"BTC\", \"DOGE\", \"DOT\", \"LINK\", \"SHIB\", \"SOL\"]:\n",
    "    df = pd.read_csv(f\"{TRAIN_DIR}/{asset}.csv\")      # Load asset's raw data\n",
    "    cross = build_cross_features(df, asset_prefix=asset)  # Create simple features for the asset\n",
    "\n",
    "    before = Xy.shape[1]      # Save current number of columns before merge\n",
    "\n",
    "    # Merge asset features with ETH features using timestamp (keep all ETH rows)\n",
    "    Xy = Xy.merge(cross, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "    # Fill in any missing data caused by merge by carrying values forward and backward\n",
    "    Xy = ffill_bfill_min(Xy)\n",
    "\n",
    "    # Print how many new columns were added and how many missing values remain\n",
    "    print(f\"Merged {asset}: +{Xy.shape[1] - before} cols → shape={Xy.shape}; NaN%={Xy.isna().mean().mean():.4f}\")\n",
    "\n",
    "    # Clean up memory by deleting temporary dataframes and running garbage collection\n",
    "    del df, cross\n",
    "    gc.collect()\n",
    "\n",
    "# ==== Final data cleanup and prepare for modeling ====\n",
    "# Replace infinite values with NaN, then fill all NaNs with zero\n",
    "Xy = Xy.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# Extract target variable (what we want to predict)\n",
    "y = Xy[\"label\"].astype(DTYPE_FLOAT).values\n",
    "\n",
    "# Select all columns except 'timestamp' and 'label' as input features\n",
    "feature_cols = [c for c in Xy.columns if c not in [\"timestamp\", \"label\"]]\n",
    "X = Xy[feature_cols].astype(DTYPE_FLOAT).values  # Convert features to numpy array for model\n",
    "\n",
    "print(\"Final train X shape:\", X.shape, \" y shape:\", y.shape)  # Show final shapes of data\n",
    "\n",
    "# Quick check to ensure there are no missing or infinite values anywhere\n",
    "assert np.isfinite(X).all() and np.isfinite(y).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff9253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.647864217456325e-05\n",
      "RMSE: 3.6195230340358164e-05\n",
      "RMSE: 3.621968854342578e-05\n",
      "RMSE: 3.613182701149182e-05\n",
      "RMSE: 3.603873021470018e-05\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ==== Define a function to create a new XGBoost regression model ====\n",
    "def make_reg():\n",
    "    return XGBRegressor(\n",
    "        n_estimators=5000,          # Train up to 5000 trees\n",
    "        learning_rate=0.05,         # How fast the model learns each step (small = slower but stable)\n",
    "        max_depth=6,                # Max depth of each tree to control complexity\n",
    "        subsample=0.8,              # Use 80% of data for each tree (helps prevent overfitting)\n",
    "        colsample_bytree=0.8,       # Use 80% of features per tree (feature randomness)\n",
    "        random_state=42,            # For reproducibility\n",
    "        tree_method=\"hist\",         # Fast histogram-based tree growing (efficient)\n",
    "        eval_metric=\"rmse\",         # Evaluation metric to monitor (Root Mean Squared Error)\n",
    "        early_stopping_rounds=200   # Stop training if no improvement after 200 rounds\n",
    "    )\n",
    "\n",
    "# ==== Prepare cross-validation ====\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold split with shuffling and fixed seed\n",
    "\n",
    "# ==== Train and validate model on each fold ====\n",
    "for tr_idx, va_idx in kf.split(X):\n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "    X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "    # Create a fresh model instance\n",
    "    model = make_reg()\n",
    "\n",
    "    # Train model on training data and validate on validation data\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],  # Validation data to monitor performance during training\n",
    "        verbose=False             # No output during training (quiet mode)\n",
    "    )\n",
    "\n",
    "    # Predict on validation data\n",
    "    y_hat = model.predict(X_va)\n",
    "\n",
    "    # Calculate RMSE (root mean squared error) as a performance measure\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_va, y_hat)))\n",
    "    print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final model trained on all data\n",
      "ETH test feat shape: (270548, 28)\n",
      "[TEST] merged BTC: +5 cols → shape=(270548, 33)\n",
      "[TEST] merged DOGE: +5 cols → shape=(270548, 38)\n",
      "[TEST] merged DOT: +5 cols → shape=(270548, 43)\n",
      "[TEST] merged LINK: +5 cols → shape=(270548, 48)\n",
      "[TEST] merged SHIB: +5 cols → shape=(270548, 53)\n",
      "[TEST] merged SOL: +5 cols → shape=(270548, 58)\n",
      "✅ Saved: C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\n",
      "timestamp      int64\n",
      "labels       float32\n",
      "dtype: object\n",
      "   timestamp    labels\n",
      "0          1  0.000024\n",
      "1          2  0.000024\n",
      "2          3  0.000050\n",
      "3          4  0.000041\n",
      "4          5  0.000062\n"
     ]
    }
   ],
   "source": [
    "# ==== Train the final model on all data ====\n",
    "# We pick 2000 trees as a safe number based on previous cross-validation results\n",
    "best_nround = 2000  \n",
    "\n",
    "final_model = XGBRegressor(\n",
    "    n_estimators=best_nround,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "final_model.fit(X, y, verbose=False)  # Train using all the data we have\n",
    "print(\"✅ Final model trained on all data\")\n",
    "\n",
    "# ==== Prepare ETH test data features ====\n",
    "eth_test = pd.read_csv(TEST_DIR + \"/ETH.csv\")  # Load ETH test dataset\n",
    "eth_test_feat = build_eth_features(eth_test)  # Create features like we did for training\n",
    "\n",
    "# Drop 'labels' column if it exists (usually test data has no labels)\n",
    "if \"labels\" in eth_test_feat.columns:\n",
    "    eth_test_feat = eth_test_feat.drop(columns=[\"labels\"])\n",
    "\n",
    "print(\"ETH test feat shape:\", eth_test_feat.shape)  # Show test features shape\n",
    "\n",
    "# ==== Add features from other assets to test data ====\n",
    "Xtest = eth_test_feat.copy()\n",
    "\n",
    "for asset in [\"BTC\", \"DOGE\", \"DOT\", \"LINK\", \"SHIB\", \"SOL\"]:\n",
    "    df = pd.read_csv(f\"{TRAIN_DIR}/{asset}.csv\")       # Load training data for the asset\n",
    "    cross = build_cross_features(df, asset_prefix=asset)  # Create compact features\n",
    "\n",
    "    before = Xtest.shape[1]   # Columns count before merge\n",
    "\n",
    "    # Merge asset features with ETH test features on timestamp\n",
    "    Xtest = Xtest.merge(cross, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "    # Fill missing data created by merge\n",
    "    Xtest = ffill_bfill_min(Xtest)\n",
    "\n",
    "    print(f\"[TEST] merged {asset}: +{Xtest.shape[1] - before} cols → shape={Xtest.shape}\")\n",
    "\n",
    "    # Free memory\n",
    "    del df, cross\n",
    "    gc.collect()\n",
    "\n",
    "# Replace infinite values with NaN, then fill all NaNs with zero\n",
    "Xtest = Xtest.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ==== Align test features with training features ====\n",
    "test_feature_cols = [c for c in Xtest.columns if c != \"timestamp\"]\n",
    "\n",
    "# Add missing columns in test set with zeros to match training features\n",
    "for c in feature_cols:\n",
    "    if c not in test_feature_cols:\n",
    "        Xtest[c] = 0.0\n",
    "\n",
    "# Remove extra columns not present during training\n",
    "extra = [c for c in test_feature_cols if c not in feature_cols]\n",
    "if extra:\n",
    "    Xtest = Xtest.drop(columns=extra)\n",
    "\n",
    "# Reorder columns to match training data exactly (timestamp first)\n",
    "Xtest = Xtest[[\"timestamp\"] + feature_cols]\n",
    "\n",
    "# Convert feature columns to numpy array for prediction\n",
    "Xt = Xtest[feature_cols].astype(DTYPE_FLOAT).values\n",
    "\n",
    "# ==== Make predictions and save results ====\n",
    "pred = final_model.predict(Xt).astype(np.float32)  # Predict using the trained model\n",
    "\n",
    "# Keep timestamps as integers (convert datetime if needed)\n",
    "timestamps = Xtest[\"timestamp\"]\n",
    "if not np.issubdtype(timestamps.dtype, np.integer):\n",
    "    timestamps = pd.to_datetime(timestamps).astype(\"int64\")  # Convert datetime to nanoseconds since epoch\n",
    "\n",
    "# Prepare final submission DataFrame with timestamp and predicted labels\n",
    "submission_final = pd.DataFrame({\n",
    "    \"timestamp\": timestamps.astype(\"int64\"),\n",
    "    \"labels\": pred\n",
    "}).sort_values(\"timestamp\")  # Sort by timestamp for consistency\n",
    "\n",
    "# Save submission file to specified path\n",
    "save_path = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\"\n",
    "submission_final.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"✅ Saved:\", save_path)\n",
    "print(submission_final.dtypes)  # Print data types to confirm correctness\n",
    "print(submission_final.head())  # Show first few rows of submission file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
