{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18672c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data: train=(631292, 23), test=(270548, 22), btc=(631292, 22)\n",
      "Converting timestamps from text to a numerical format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:105: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating data to ensure unique timestamps...\n",
      "Timestamp conversion and de-duplication complete.\n",
      "Feature engineering and merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:154: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:155: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for training with 31 features.\n",
      "\n",
      "Starting cross-validation for base models...\n",
      "--- Fold 1 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6943\n",
      "[LightGBM] [Info] Number of data points in the train set: 105217, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000063\n",
      "--- Fold 2 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6953\n",
      "[LightGBM] [Info] Number of data points in the train set: 210432, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000060\n",
      "--- Fold 3 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6949\n",
      "[LightGBM] [Info] Number of data points in the train set: 315647, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6951\n",
      "[LightGBM] [Info] Number of data points in the train set: 420862, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6955\n",
      "[LightGBM] [Info] Number of data points in the train set: 526077, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000057\n",
      "\n",
      "--- Training Stacking Meta-Model ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "\n",
      "-------------------------------------------\n",
      "LightGBM CV Pearson : 0.462816\n",
      "CatBoost CV Pearson : 0.497202\n",
      "Stacked CV Pearson  : 0.632987 ✨\n",
      "-------------------------------------------\n",
      "\n",
      "Training final base models on all data...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6957\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "Generating final predictions with the trained meta-model...\n",
      "\n",
      "Stacked ensemble submission successfully saved to: C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds a final, advanced stacked ensemble model for the GoQuant competition.\n",
    "\n",
    "This version uses a non-linear meta-model to intelligently combine predictions\n",
    "and includes a full suite of robust features.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# File Paths \n",
    "ETH_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\ETH.csv\"\n",
    "ETH_TEST_PATH  = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\test\\ETH.csv\"\n",
    "BTC_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\BTC.csv\"\n",
    "SUB_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission.csv\"\n",
    "OUT_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\"\n",
    "\n",
    "# Base Model \n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression_l2',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 64,\n",
    "    'min_child_samples': 100,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'n_estimators': 10_000,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 10000,\n",
    "    'learning_rate': 0.02,\n",
    "    'depth': 8,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 400\n",
    "}\n",
    "\n",
    "# Meta-Model Parameters \n",
    "META_MODEL_PARAMS = {\n",
    "    'objective': 'regression_l2',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 16,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "\n",
    "def create_features(df: pd.DataFrame, is_primary_asset=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if is_primary_asset:\n",
    "        df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['hour'] = df['timestamp_dt'].dt.hour\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "\n",
    "    df['wap'] = ((df['bid_price1'] * df['ask_volume1']) + (df['ask_price1'] * df['bid_volume1'])) / \\\n",
    "                (df['bid_volume1'] + df['ask_volume1'])\n",
    "    df['log_return'] = np.log(df['wap'] / df['wap'].shift(1))\n",
    "    \n",
    "    if is_primary_asset:\n",
    "        df['bid_ask_spread'] = df['ask_price1'] - df['bid_price1']\n",
    "        df['total_volume'] = df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) + \\\n",
    "                             df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "        df['full_obi'] = (df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) - \\\n",
    "                          df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)) / df['total_volume']\n",
    "        df['wap_diff_10'] = df['wap'].diff(10)\n",
    "\n",
    "    df['realized_vol_100'] = df['log_return'].rolling(window=100).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_prepare_data(eth_train_path, eth_test_path, btc_train_path, sub_path):\n",
    "    \"\"\"Loads, de-duplicates, cleans, and merges all data with robust timestamp handling.\"\"\"\n",
    "    # Load all raw data\n",
    "    train_df = pd.read_csv(eth_train_path)\n",
    "    test_df  = pd.read_csv(eth_test_path)\n",
    "    btc_df   = pd.read_csv(btc_train_path)\n",
    "    submission_df = pd.read_csv(sub_path)\n",
    "    print(f\"Loaded raw data: train={train_df.shape}, test={test_df.shape}, btc={btc_df.shape}\")\n",
    "\n",
    "    print(\"Converting timestamps from text to a numerical format...\")\n",
    "    for df in [train_df, test_df, btc_df, submission_df]:\n",
    "        if 'timestamp' in df.columns:\n",
    "      \n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Drop any rows where timestamp conversion failed\n",
    "    train_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    test_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    btc_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    \n",
    "    # changed for MemoryError: Aggregate by timestamp to remove duplicates \n",
    "    print(\"Aggregating data to ensure unique timestamps...\")\n",
    "    train_df = train_df.groupby('timestamp').mean().reset_index()\n",
    "    test_df  = test_df.groupby('timestamp').mean().reset_index()\n",
    "    btc_df   = btc_df.groupby('timestamp').mean().reset_index()\n",
    "    \n",
    "   \n",
    "    # Convert the clean datetime objects to Unix timestamp integers.\n",
    "    for df in [train_df, test_df, btc_df]:\n",
    "        df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n",
    "        \n",
    "    print(\"Timestamp conversion and de-duplication complete.\")\n",
    "\n",
    "    \n",
    "    train_featured = create_features(train_df, is_primary_asset=True)\n",
    "    test_featured  = create_features(test_df, is_primary_asset=True)\n",
    "    btc_featured   = create_features(btc_df, is_primary_asset=False)\n",
    "\n",
    "    # Merge cross-asset features\n",
    "    btc_features_to_merge = btc_featured[['timestamp', 'log_return', 'realized_vol_100']].rename(\n",
    "        columns={'log_return': 'log_return_btc', 'realized_vol_100': 'realized_vol_100_btc'}\n",
    "    )\n",
    "    train_featured = pd.merge(train_featured, btc_features_to_merge, on='timestamp', how='left')\n",
    "    test_featured  = pd.merge(test_featured, btc_features_to_merge, on='timestamp', how='left')\n",
    "    \n",
    "    # rolling correlation\n",
    "    train_featured['eth_btc_corr_100'] = train_featured['log_return'].rolling(window=100).corr(train_featured['log_return_btc'])\n",
    "    test_featured['eth_btc_corr_100']  = test_featured['log_return'].rolling(window=100).corr(test_featured['log_return_btc'])\n",
    "    print(\"Feature engineering and merging complete.\")\n",
    "    \n",
    "    # Prepare Final Model Matrices\n",
    "    TARGET_COL = 'label'\n",
    "    features_to_drop = ['timestamp', 'timestamp_dt', 'hour', TARGET_COL, 'log_return', 'log_return_btc']\n",
    "    MODEL_FEATURES = [col for col in train_featured.columns if col not in features_to_drop]\n",
    "\n",
    "    X = train_featured[MODEL_FEATURES]\n",
    "    y = train_featured[TARGET_COL]\n",
    "    X_test = test_featured[MODEL_FEATURES]\n",
    "\n",
    "    # Final cleanup\n",
    "    for df in [X, X_test]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "    \n",
    "    print(f\"Data prepared for training with {len(MODEL_FEATURES)} features.\")\n",
    "    \n",
    "    return X, y, X_test, submission_df\n",
    "X, y, X_test, submission_df = load_and_prepare_data(\n",
    "    ETH_TRAIN_PATH, ETH_TEST_PATH, BTC_TRAIN_PATH, SUB_PATH\n",
    ")\n",
    "\n",
    "time_series_splitter = TimeSeriesSplit(n_splits=5)\n",
    "lgbm_oof_preds = np.zeros(len(X))\n",
    "cat_oof_preds = np.zeros(len(X))\n",
    "\n",
    "print(\"\\nStarting cross-validation for base models...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(time_series_splitter.split(X), 1):\n",
    "    print(f\"Fold {fold} \")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(400, verbose=False)])\n",
    "    lgbm_oof_preds[val_idx] = lgbm.predict(X_val)\n",
    "\n",
    "    cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "    cat.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    cat_oof_preds[val_idx] = cat.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"\\n Training Stacking Meta-Model \")\n",
    "meta_features_train = pd.DataFrame({'lgbm_pred': lgbm_oof_preds, 'cat_pred': cat_oof_preds})\n",
    "\n",
    "# Use a simple but non-linear model to combine predictions\n",
    "meta_model = lgb.LGBMRegressor(**META_MODEL_PARAMS)\n",
    "meta_model.fit(meta_features_train, y)\n",
    "\n",
    "# Evaluate all models\n",
    "corr_lgbm_oof, _ = pearsonr(y, lgbm_oof_preds)\n",
    "corr_cat_oof, _ = pearsonr(y, cat_oof_preds)\n",
    "stacked_preds_oof = meta_model.predict(meta_features_train)\n",
    "corr_stacked_oof, _ = pearsonr(y, stacked_preds_oof)\n",
    "\n",
    "print(f\"LightGBM CV Pearson : {corr_lgbm_oof:.6f}\")\n",
    "print(f\"CatBoost CV Pearson : {corr_cat_oof:.6f}\")\n",
    "print(f\"Stacked CV Pearson  : {corr_stacked_oof:.6f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#  5. Final Prediction\n",
    "# ==============================================================================\n",
    "print(\"\\nTraining final base models on all data\")\n",
    "final_lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "final_lgbm.fit(X, y)\n",
    "lgbm_final_preds = final_lgbm.predict(X_test)\n",
    "\n",
    "final_cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "final_cat.fit(X, y)\n",
    "cat_final_preds = final_cat.predict(X_test)\n",
    "\n",
    "print(\"Generating final predictions with the trained meta-model\")\n",
    "meta_features_test = pd.DataFrame({'lgbm_pred': lgbm_final_preds, 'cat_pred': cat_final_preds})\n",
    "final_predictions = meta_model.predict(meta_features_test)\n",
    "\n",
    "submission_df['labels'] = final_predictions\n",
    "\n",
    "try:\n",
    "    submission_df.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nStacked ensemble submission successfully saved to: {OUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing ETH data ---\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train\\\\ETH.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 104\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#  3. Data Preparation Pipeline\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Loading and preparing ETH data ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mETH.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m train_featured \u001b[38;5;241m=\u001b[39m create_primary_features(train_df)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_df; gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train\\\\ETH.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the final, state-of-the-art script for the GoQuant assignment.\n",
    "\n",
    "This version integrates a Long Short-Term Memory (LSTM) neural network into the\n",
    "stacking ensemble, combining the strengths of tree-based models and sequence models.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ==============================================================================\n",
    "#  1. Configuration\n",
    "# ==============================================================================\n",
    "# --- File Paths ---\n",
    "ETH_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\ETH.csv\"\n",
    "ETH_TEST_PATH  = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\test\\ETH.csv\"\n",
    "SUB_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission.csv\"\n",
    "OUT_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\"\n",
    "\n",
    "# --- Model Parameters ---\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression_l2', 'learning_rate': 0.02, 'num_leaves': 64,\n",
    "    'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8,\n",
    "    'n_estimators': 10_000, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 10000, 'learning_rate': 0.02, 'depth': 8, 'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE', 'random_seed': 42, 'verbose': 0, 'early_stopping_rounds': 400\n",
    "}\n",
    "META_MODEL_PARAMS = {\n",
    "    'objective': 'regression_l2', 'n_estimators': 1000, 'learning_rate': 0.01,\n",
    "    'num_leaves': 16, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "# --- LSTM Configuration ---\n",
    "SEQUENCE_LENGTH = 60  # Look at the last 60 seconds of data to make a prediction\n",
    "LSTM_EPOCHS = 20\n",
    "LSTM_BATCH_SIZE = 256\n",
    "\n",
    "# ==============================================================================\n",
    "#  2. Data Preparation Pipeline\n",
    "# ==============================================================================\n",
    "def create_features(df: pd.DataFrame):\n",
    "    \"\"\"Engineers features from the ETH order book data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df['hour'] = df['timestamp_dt'].dt.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['wap'] = (df['bid_price1'] * df['ask_volume1'] + df['ask_price1'] * df['bid_volume1']) / \\\n",
    "                (df['bid_volume1'] + df['ask_volume1'])\n",
    "    df['log_return'] = np.log(df['wap'] / df['wap'].shift(1))\n",
    "    df['bid_ask_spread'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['total_volume'] = df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) + \\\n",
    "                         df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "    df['full_obi'] = (df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) - \\\n",
    "                      df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)) / df['total_volume']\n",
    "    df['wap_diff_10'] = df['wap'].diff(10)\n",
    "    df['realized_vol_100'] = df['log_return'].rolling(window=100).std()\n",
    "    return df\n",
    "\n",
    "def load_and_prepare_data(paths):\n",
    "    \"\"\"Loads and prepares the ETH data.\"\"\"\n",
    "    train_df = pd.read_csv(paths['eth_train'])\n",
    "    test_df  = pd.read_csv(paths['eth_test'])\n",
    "    submission_df = pd.read_csv(paths['sub'])\n",
    "    print(f\"Loaded raw data: train={train_df.shape}, test={test_df.shape}\")\n",
    "\n",
    "    for name, df in {'train': train_df, 'test': test_df}.items():\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df.dropna(subset=['timestamp'], inplace=True)\n",
    "        df = df.groupby('timestamp').mean().reset_index()\n",
    "        df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n",
    "        if name == 'train': train_df = df\n",
    "        else: test_df = df\n",
    "    print(\"Timestamp conversion and de-duplication complete.\")\n",
    "\n",
    "    train_featured = create_features(train_df)\n",
    "    test_featured  = create_features(test_df)\n",
    "    print(\"Feature engineering complete.\")\n",
    "    \n",
    "    TARGET_COL = 'label'\n",
    "    features_to_drop = ['timestamp', 'timestamp_dt', 'hour', TARGET_COL, 'log_return']\n",
    "    MODEL_FEATURES = [col for col in train_featured.columns if col not in features_to_drop]\n",
    "\n",
    "    X = train_featured[MODEL_FEATURES]\n",
    "    y = train_featured[TARGET_COL]\n",
    "    X_test = test_featured[MODEL_FEATURES]\n",
    "\n",
    "    for df in [X, X_test]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "    \n",
    "    if X.empty:\n",
    "        raise ValueError(\"Dataframe X is empty after preparation.\")\n",
    "        \n",
    "    print(f\"Data prepared for training with {len(MODEL_FEATURES)} features.\")\n",
    "    return X, y, X_test, submission_df\n",
    "\n",
    "def create_sequences(X, y, sequence_length=60):\n",
    "    \"\"\"Reshapes data into sequences for LSTM model.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_seq.append(X.iloc[i:(i + sequence_length)].values)\n",
    "        y_seq.append(y.iloc[i + sequence_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "paths = {'eth_train': ETH_TRAIN_PATH, 'eth_test': ETH_TEST_PATH, 'sub': SUB_PATH}\n",
    "X, y, X_test, submission_df = load_and_prepare_data(paths)\n",
    "\n",
    "# ==============================================================================\n",
    "#  3. Cross-Validation & Stacking\n",
    "# ==============================================================================\n",
    "time_series_splitter = TimeSeriesSplit(n_splits=5)\n",
    "lgbm_oof_preds = np.zeros(len(X))\n",
    "cat_oof_preds = np.zeros(len(X))\n",
    "lstm_oof_preds = np.zeros(len(X))\n",
    "\n",
    "print(\"\\nStarting cross-validation for base models...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(time_series_splitter.split(X), 1):\n",
    "    print(f\"--- Fold {fold} ---\")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # --- Train LightGBM & CatBoost ---\n",
    "    print(\"  Training tree models...\")\n",
    "    lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(400, verbose=False)])\n",
    "    lgbm_oof_preds[val_idx] = lgbm.predict(X_val)\n",
    "\n",
    "    cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "    cat.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    cat_oof_preds[val_idx] = cat.predict(X_val)\n",
    "    \n",
    "    # --- Train LSTM ---\n",
    "    print(\"  Preparing sequences for LSTM...\")\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQUENCE_LENGTH)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQUENCE_LENGTH)\n",
    "    \n",
    "    print(\"  Training LSTM...\")\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    lstm_model.compile(optimizer='adam', loss='mse')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    lstm_model.fit(X_train_seq, y_train_seq,\n",
    "                   validation_data=(X_val_seq, y_val_seq),\n",
    "                   epochs=LSTM_EPOCHS,\n",
    "                   batch_size=LSTM_BATCH_SIZE,\n",
    "                   callbacks=[early_stop],\n",
    "                   verbose=0)\n",
    "    \n",
    "    # LSTM predictions need to be aligned with the validation set\n",
    "    lstm_preds = lstm_model.predict(X_val_seq).flatten()\n",
    "    lstm_oof_preds[val_idx[SEQUENCE_LENGTH:]] = lstm_preds\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Stacking Meta-Model ---\")\n",
    "meta_features_train = pd.DataFrame({\n",
    "    'lgbm_pred': lgbm_oof_preds,\n",
    "    'cat_pred': cat_oof_preds,\n",
    "    'lstm_pred': lstm_oof_preds\n",
    "})\n",
    "oof_mask = lgbm_oof_preds != 0 # Use a mask from a tree model\n",
    "meta_model = lgb.LGBMRegressor(**META_MODEL_PARAMS)\n",
    "meta_model.fit(meta_features_train[oof_mask], y[oof_mask])\n",
    "\n",
    "# ==============================================================================\n",
    "#  4. Evaluation & Final Prediction\n",
    "# ==============================================================================\n",
    "y_for_oof_eval = y[oof_mask]\n",
    "lgbm_oof_eval = lgbm_oof_preds[oof_mask]\n",
    "cat_oof_eval = cat_oof_preds[oof_mask]\n",
    "# Align LSTM predictions for evaluation\n",
    "lstm_oof_eval = lstm_oof_preds[oof_mask]\n",
    "\n",
    "stacked_preds_oof = meta_model.predict(meta_features_train[oof_mask])\n",
    "\n",
    "corr_lgbm_oof, _ = pearsonr(y_for_oof_eval, lgbm_oof_eval)\n",
    "corr_cat_oof, _ = pearsonr(y_for_oof_eval, cat_oof_eval)\n",
    "corr_lstm_oof, _ = pearsonr(y_for_oof_eval, lstm_oof_eval)\n",
    "corr_stacked_oof, _ = pearsonr(y_for_oof_eval, stacked_preds_oof)\n",
    "\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(f\"LightGBM CV Pearson : {corr_lgbm_oof:.6f}\")\n",
    "print(f\"CatBoost CV Pearson : {corr_cat_oof:.6f}\")\n",
    "print(f\"LSTM CV Pearson     : {corr_lstm_oof:.6f}\")\n",
    "print(f\"Stacked CV Pearson  : {corr_stacked_oof:.6f} ✨\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "# --- Final Model Training ---\n",
    "print(\"\\nTraining final base models on all data...\")\n",
    "final_lgbm = lgb.LGBMRegressor(**LGBM_PARAMS).fit(X, y)\n",
    "lgbm_final_preds = final_lgbm.predict(X_test)\n",
    "\n",
    "final_cat = CatBoostRegressor(**CATBOOST_PARAMS).fit(X, y)\n",
    "cat_final_preds = final_cat.predict(X_test)\n",
    "\n",
    "print(\"Training final LSTM on recent data...\")\n",
    "X_train_seq_final, y_train_seq_final = create_sequences(X, y, SEQUENCE_LENGTH)\n",
    "final_lstm_model = Sequential([\n",
    "    Input(shape=(X_train_seq_final.shape[1], X_train_seq_final.shape[2])),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(1)\n",
    "])\n",
    "final_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "final_lstm_model.fit(X_train_seq_final, y_train_seq_final, epochs=LSTM_EPOCHS, batch_size=LSTM_BATCH_SIZE, verbose=0)\n",
    "\n",
    "# For final LSTM prediction, we need the last `SEQUENCE_LENGTH` steps of the training data\n",
    "X_test_seq_final = []\n",
    "X_test_with_history = pd.concat([X.iloc[-SEQUENCE_LENGTH:], X_test])\n",
    "for i in range(len(X_test)):\n",
    "    X_test_seq_final.append(X_test_with_history.iloc[i:(i + SEQUENCE_LENGTH)].values)\n",
    "lstm_final_preds = final_lstm_model.predict(np.array(X_test_seq_final)).flatten()\n",
    "\n",
    "\n",
    "print(\"Generating final predictions with the trained meta-model...\")\n",
    "meta_features_test = pd.DataFrame({\n",
    "    'lgbm_pred': lgbm_final_preds,\n",
    "    'cat_pred': cat_final_preds,\n",
    "    'lstm_pred': lstm_final_preds\n",
    "})\n",
    "final_predictions = meta_model.predict(meta_features_test)\n",
    "\n",
    "submission_df['labels'] = final_predictions\n",
    "\n",
    "try:\n",
    "    submission_df.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nStacked ensemble submission successfully saved to: {OUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving submission file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
