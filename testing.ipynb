{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18672c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data: train=(631292, 23), test=(270548, 22), btc=(631292, 22)\n",
      "Converting timestamps from text to a numerical format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:105: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating data to ensure unique timestamps...\n",
      "Timestamp conversion and de-duplication complete.\n",
      "Feature engineering and merging complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:154: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:155: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\194826311.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for training with 31 features.\n",
      "\n",
      "Starting cross-validation for base models...\n",
      "--- Fold 1 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6943\n",
      "[LightGBM] [Info] Number of data points in the train set: 105217, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000063\n",
      "--- Fold 2 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6953\n",
      "[LightGBM] [Info] Number of data points in the train set: 210432, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000060\n",
      "--- Fold 3 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6949\n",
      "[LightGBM] [Info] Number of data points in the train set: 315647, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6951\n",
      "[LightGBM] [Info] Number of data points in the train set: 420862, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 5 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6955\n",
      "[LightGBM] [Info] Number of data points in the train set: 526077, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000057\n",
      "\n",
      "--- Training Stacking Meta-Model ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "\n",
      "-------------------------------------------\n",
      "LightGBM CV Pearson : 0.462816\n",
      "CatBoost CV Pearson : 0.497202\n",
      "Stacked CV Pearson  : 0.632987 âœ¨\n",
      "-------------------------------------------\n",
      "\n",
      "Training final base models on all data...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6957\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "Generating final predictions with the trained meta-model...\n",
      "\n",
      "Stacked ensemble submission successfully saved to: C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds a final, advanced stacked ensemble model for the GoQuant competition.\n",
    "\n",
    "This version uses a non-linear meta-model to intelligently combine predictions\n",
    "and includes a full suite of robust features.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# File Paths \n",
    "ETH_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\ETH.csv\"\n",
    "ETH_TEST_PATH  = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\test\\ETH.csv\"\n",
    "BTC_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\BTC.csv\"\n",
    "SUB_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission.csv\"\n",
    "OUT_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\"\n",
    "\n",
    "# Base Model \n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression_l2',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 64,\n",
    "    'min_child_samples': 100,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'n_estimators': 10_000,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 10000,\n",
    "    'learning_rate': 0.02,\n",
    "    'depth': 8,\n",
    "    'loss_function': 'RMSE',\n",
    "    'eval_metric': 'RMSE',\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_rounds': 400\n",
    "}\n",
    "\n",
    "# Meta-Model Parameters \n",
    "META_MODEL_PARAMS = {\n",
    "    'objective': 'regression_l2',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 16,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "\n",
    "def create_features(df: pd.DataFrame, is_primary_asset=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if is_primary_asset:\n",
    "        df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "        df['hour'] = df['timestamp_dt'].dt.hour\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "\n",
    "    df['wap'] = ((df['bid_price1'] * df['ask_volume1']) + (df['ask_price1'] * df['bid_volume1'])) / \\\n",
    "                (df['bid_volume1'] + df['ask_volume1'])\n",
    "    df['log_return'] = np.log(df['wap'] / df['wap'].shift(1))\n",
    "    \n",
    "    if is_primary_asset:\n",
    "        df['bid_ask_spread'] = df['ask_price1'] - df['bid_price1']\n",
    "        df['total_volume'] = df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) + \\\n",
    "                             df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "        df['full_obi'] = (df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) - \\\n",
    "                          df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)) / df['total_volume']\n",
    "        df['wap_diff_10'] = df['wap'].diff(10)\n",
    "\n",
    "    df['realized_vol_100'] = df['log_return'].rolling(window=100).std()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_prepare_data(eth_train_path, eth_test_path, btc_train_path, sub_path):\n",
    "    \"\"\"Loads, de-duplicates, cleans, and merges all data with robust timestamp handling.\"\"\"\n",
    "    # Load all raw data\n",
    "    train_df = pd.read_csv(eth_train_path)\n",
    "    test_df  = pd.read_csv(eth_test_path)\n",
    "    btc_df   = pd.read_csv(btc_train_path)\n",
    "    submission_df = pd.read_csv(sub_path)\n",
    "    print(f\"Loaded raw data: train={train_df.shape}, test={test_df.shape}, btc={btc_df.shape}\")\n",
    "\n",
    "    print(\"Converting timestamps from text to a numerical format...\")\n",
    "    for df in [train_df, test_df, btc_df, submission_df]:\n",
    "        if 'timestamp' in df.columns:\n",
    "      \n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Drop any rows where timestamp conversion failed\n",
    "    train_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    test_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    btc_df.dropna(subset=['timestamp'], inplace=True)\n",
    "    \n",
    "    # changed for MemoryError: Aggregate by timestamp to remove duplicates \n",
    "    print(\"Aggregating data to ensure unique timestamps...\")\n",
    "    train_df = train_df.groupby('timestamp').mean().reset_index()\n",
    "    test_df  = test_df.groupby('timestamp').mean().reset_index()\n",
    "    btc_df   = btc_df.groupby('timestamp').mean().reset_index()\n",
    "    \n",
    "   \n",
    "    # Convert the clean datetime objects to Unix timestamp integers.\n",
    "    for df in [train_df, test_df, btc_df]:\n",
    "        df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n",
    "        \n",
    "    print(\"Timestamp conversion and de-duplication complete.\")\n",
    "\n",
    "    \n",
    "    train_featured = create_features(train_df, is_primary_asset=True)\n",
    "    test_featured  = create_features(test_df, is_primary_asset=True)\n",
    "    btc_featured   = create_features(btc_df, is_primary_asset=False)\n",
    "\n",
    "    # Merge cross-asset features\n",
    "    btc_features_to_merge = btc_featured[['timestamp', 'log_return', 'realized_vol_100']].rename(\n",
    "        columns={'log_return': 'log_return_btc', 'realized_vol_100': 'realized_vol_100_btc'}\n",
    "    )\n",
    "    train_featured = pd.merge(train_featured, btc_features_to_merge, on='timestamp', how='left')\n",
    "    test_featured  = pd.merge(test_featured, btc_features_to_merge, on='timestamp', how='left')\n",
    "    \n",
    "    # rolling correlation\n",
    "    train_featured['eth_btc_corr_100'] = train_featured['log_return'].rolling(window=100).corr(train_featured['log_return_btc'])\n",
    "    test_featured['eth_btc_corr_100']  = test_featured['log_return'].rolling(window=100).corr(test_featured['log_return_btc'])\n",
    "    print(\"Feature engineering and merging complete.\")\n",
    "    \n",
    "    # Prepare Final Model Matrices\n",
    "    TARGET_COL = 'label'\n",
    "    features_to_drop = ['timestamp', 'timestamp_dt', 'hour', TARGET_COL, 'log_return', 'log_return_btc']\n",
    "    MODEL_FEATURES = [col for col in train_featured.columns if col not in features_to_drop]\n",
    "\n",
    "    X = train_featured[MODEL_FEATURES]\n",
    "    y = train_featured[TARGET_COL]\n",
    "    X_test = test_featured[MODEL_FEATURES]\n",
    "\n",
    "    # Final cleanup\n",
    "    for df in [X, X_test]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "    \n",
    "    print(f\"Data prepared for training with {len(MODEL_FEATURES)} features.\")\n",
    "    \n",
    "    return X, y, X_test, submission_df\n",
    "X, y, X_test, submission_df = load_and_prepare_data(\n",
    "    ETH_TRAIN_PATH, ETH_TEST_PATH, BTC_TRAIN_PATH, SUB_PATH\n",
    ")\n",
    "\n",
    "time_series_splitter = TimeSeriesSplit(n_splits=5)\n",
    "lgbm_oof_preds = np.zeros(len(X))\n",
    "cat_oof_preds = np.zeros(len(X))\n",
    "\n",
    "print(\"\\nStarting cross-validation for base models...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(time_series_splitter.split(X), 1):\n",
    "    print(f\"Fold {fold} \")\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(400, verbose=False)])\n",
    "    lgbm_oof_preds[val_idx] = lgbm.predict(X_val)\n",
    "\n",
    "    cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "    cat.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    cat_oof_preds[val_idx] = cat.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"\\n Training Stacking Meta-Model \")\n",
    "meta_features_train = pd.DataFrame({'lgbm_pred': lgbm_oof_preds, 'cat_pred': cat_oof_preds})\n",
    "\n",
    "# Use a simple but non-linear model to combine predictions\n",
    "meta_model = lgb.LGBMRegressor(**META_MODEL_PARAMS)\n",
    "meta_model.fit(meta_features_train, y)\n",
    "\n",
    "# Evaluate all models\n",
    "corr_lgbm_oof, _ = pearsonr(y, lgbm_oof_preds)\n",
    "corr_cat_oof, _ = pearsonr(y, cat_oof_preds)\n",
    "stacked_preds_oof = meta_model.predict(meta_features_train)\n",
    "corr_stacked_oof, _ = pearsonr(y, stacked_preds_oof)\n",
    "\n",
    "print(f\"LightGBM CV Pearson : {corr_lgbm_oof:.6f}\")\n",
    "print(f\"CatBoost CV Pearson : {corr_cat_oof:.6f}\")\n",
    "print(f\"Stacked CV Pearson  : {corr_stacked_oof:.6f}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#  5. Final Prediction\n",
    "# ==============================================================================\n",
    "print(\"\\nTraining final base models on all data\")\n",
    "final_lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "final_lgbm.fit(X, y)\n",
    "lgbm_final_preds = final_lgbm.predict(X_test)\n",
    "\n",
    "final_cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "final_cat.fit(X, y)\n",
    "cat_final_preds = final_cat.predict(X_test)\n",
    "\n",
    "print(\"Generating final predictions with the trained meta-model\")\n",
    "meta_features_test = pd.DataFrame({'lgbm_pred': lgbm_final_preds, 'cat_pred': cat_final_preds})\n",
    "final_predictions = meta_model.predict(meta_features_test)\n",
    "\n",
    "submission_df['labels'] = final_predictions\n",
    "\n",
    "try:\n",
    "    submission_df.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nStacked ensemble submission successfully saved to: {OUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving submission file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c6a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data: train=(631292, 23), test=(270548, 22)\n",
      "Timestamp conversion and de-duplication complete.\n",
      "Feature engineering complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\3207352438.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\3207352438.py:106: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\3207352438.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\3207352438.py:107: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20444\\3207352438.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for training with 29 features.\n",
      "\n",
      "Starting cross-validation for base models...\n",
      "--- Fold 1 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6688\n",
      "[LightGBM] [Info] Number of data points in the train set: 105217, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000063\n",
      "--- Fold 2 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6698\n",
      "[LightGBM] [Info] Number of data points in the train set: 210432, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000060\n",
      "--- Fold 3 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019372 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6694\n",
      "[LightGBM] [Info] Number of data points in the train set: 315647, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 4 ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6696\n",
      "[LightGBM] [Info] Number of data points in the train set: 420862, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "--- Fold 5 ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6700\n",
      "[LightGBM] [Info] Number of data points in the train set: 526077, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000057\n",
      "\n",
      "--- Training Stacking Meta-Model ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "\n",
      "--- Evaluating OOF Predictions ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 510\n",
      "[LightGBM] [Info] Number of data points in the train set: 526075, number of used features: 2\n",
      "[LightGBM] [Info] Start training from score 0.000065\n",
      "\n",
      "-------------------------------------------\n",
      "LightGBM CV Pearson : 0.614861\n",
      "CatBoost CV Pearson : 0.646609\n",
      "Stacked CV Pearson  : 0.665573 âœ¨\n",
      "-------------------------------------------\n",
      "\n",
      "Training final base models on all data...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6702\n",
      "[LightGBM] [Info] Number of data points in the train set: 631292, number of used features: 29\n",
      "[LightGBM] [Info] Start training from score 0.000064\n",
      "Generating final predictions with the trained meta-model...\n",
      "\n",
      "Stacked ensemble submission successfully saved to: C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# GoQuant Implied Volatility Forecasting\n",
    "# Final Submission Notebook\n",
    "#\n",
    "# Author: [Your Name]\n",
    "# Kaggle Username: [Your Kaggle Username]\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# ## 1. My Approach & Objective\n",
    "#\n",
    "# This is my final notebook for the GoQuant assignment. My goal is to forecast\n",
    "# the 10-second-ahead implied volatility for ETH using the provided order book data.\n",
    "#\n",
    "# After a lot of experimenting, I landed on a stacking ensemble. It combines\n",
    "# LightGBM and CatBoost and gave me the best and most stable cross-validation\n",
    "# score on the official Pearson Correlation metric.\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ==============================================================================\n",
    "# ## 2. Configuration\n",
    "#\n",
    "# I'm putting all my file paths and model parameters here so they're easy to change later.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Paths ---\n",
    "# Pointing to my local data files.\n",
    "ETH_TRAIN_PATH = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\train\\ETH.csv\"\n",
    "ETH_TEST_PATH  = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\test\\ETH.csv\"\n",
    "SUB_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-impl-volatility-forecasting\\submission.csv\"\n",
    "OUT_PATH   = r\"C:\\Users\\HP\\Downloads\\gq-implied-volatility-forecasting\\submission_final.csv\"\n",
    "\n",
    "# --- Base Model Parameters ---\n",
    "# These are the settings for my two main models. I tuned them to be a bit simpler\n",
    "# and more robust to avoid overfitting on the noisy data.\n",
    "LGBM_PARAMS = {\n",
    "    'objective': 'regression_l2', 'learning_rate': 0.02, 'num_leaves': 64,\n",
    "    'min_child_samples': 100, 'colsample_bytree': 0.8, 'subsample': 0.8,\n",
    "    'n_estimators': 10_000, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "CATBOOST_PARAMS = {\n",
    "    'iterations': 10000, 'learning_rate': 0.02, 'depth': 8,\n",
    "    'loss_function': 'RMSE', 'eval_metric': 'RMSE',\n",
    "    'random_seed': 42, 'verbose': 0, 'early_stopping_rounds': 400\n",
    "}\n",
    "\n",
    "# --- Meta-Model Parameters (for Stacking) ---\n",
    "# This is a simpler model that just learns how to best combine the predictions\n",
    "# from the two base models.\n",
    "META_MODEL_PARAMS = {\n",
    "    'objective': 'regression_l2', 'n_estimators': 1000,\n",
    "    'learning_rate': 0.01, 'num_leaves': 16,\n",
    "    'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# ## 3. Data Prep & Feature Engineering\n",
    "#\n",
    "# This is where most of the work happened. I'm loading the data, cleaning it,\n",
    "# and creating a bunch of features to try and capture what's happening in the market.\n",
    "# ==============================================================================\n",
    "\n",
    "def create_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"This function is where I create all my features for a given dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- Time Features ---\n",
    "    # The hour of the day seems important for volatility, but I need to encode it\n",
    "    # cyclically so the model understands hour 23 is close to hour 0.\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    df['hour'] = df['timestamp_dt'].dt.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "\n",
    "    # --- Core Price & Volume Features ---\n",
    "    # WAP is way better than mid-price because it accounts for volume. This is my main price signal.\n",
    "    df['wap'] = (df['bid_price1'] * df['ask_volume1'] + df['ask_price1'] * df['bid_volume1']) / \\\n",
    "                (df['bid_volume1'] + df['ask_volume1'])\n",
    "    \n",
    "    # Using log returns because they are standard for financial data.\n",
    "    df['log_return'] = np.log(df['wap'] / df['wap'].shift(1))\n",
    "    \n",
    "    # Basic features for market liquidity and pressure.\n",
    "    df['bid_ask_spread'] = df['ask_price1'] - df['bid_price1']\n",
    "    df['total_volume'] = df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) + \\\n",
    "                         df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "    df['full_obi'] = (df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1) - \\\n",
    "                      df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)) / df['total_volume']\n",
    "    \n",
    "    # --- Momentum & Volatility Features ---\n",
    "    # Adding some features to capture recent trends.\n",
    "    df['wap_diff_10'] = df['wap'].diff(10) # How much has the price changed in 10 seconds?\n",
    "    df['realized_vol_100'] = df['log_return'].rolling(window=100).std() # Volatility over the last ~1.5 mins.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_prepare_data(paths: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    This is my main data pipeline. It handles loading, cleaning, and feature creation all in one place.\n",
    "    I built this to be robust after running into a bunch of data errors.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Preparation ---\")\n",
    "    train_df = pd.read_csv(paths['eth_train'])\n",
    "    test_df  = pd.read_csv(paths['eth_test'])\n",
    "    submission_df = pd.read_csv(paths['sub'])\n",
    "    print(f\"Loaded raw data: train={train_df.shape}, test={test_df.shape}\")\n",
    "\n",
    "    # --- De-duplication and Timestamp Conversion ---\n",
    "    # This was a critical step. The raw data had duplicate timestamps which caused memory errors.\n",
    "    # I'm grouping by timestamp to get a clean 1-second interval.\n",
    "    for name, df in {'train': train_df, 'test': test_df}.items():\n",
    "        # Using to_datetime because the timestamps are date strings.\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df.dropna(subset=['timestamp'], inplace=True)\n",
    "        \n",
    "        df = df.groupby('timestamp').mean().reset_index()\n",
    "        # Finally, converting the clean datetime to a simple number (Unix timestamp).\n",
    "        df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n",
    "        \n",
    "        if name == 'train': train_df = df\n",
    "        else: test_df = df\n",
    "    print(\"Timestamp conversion and de-duplication complete.\")\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "    train_featured = create_features(train_df)\n",
    "    test_featured  = create_features(test_df)\n",
    "    print(\"Feature engineering complete.\")\n",
    "    \n",
    "    # --- Final Data Matrices ---\n",
    "    # Getting my X and y ready for the models.\n",
    "    TARGET_COL = 'label'\n",
    "    features_to_drop = ['timestamp', 'timestamp_dt', 'hour', TARGET_COL, 'log_return']\n",
    "    MODEL_FEATURES = [col for col in train_featured.columns if col not in features_to_drop]\n",
    "\n",
    "    X = train_featured[MODEL_FEATURES]\n",
    "    y = train_featured[TARGET_COL]\n",
    "    X_test = test_featured[MODEL_FEATURES]\n",
    "\n",
    "    # Final cleanup of any NaNs created by rolling features.\n",
    "    for df in [X, X_test]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(method='bfill', inplace=True)\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "    \n",
    "    if X.empty:\n",
    "        raise ValueError(\"Dataframe X is empty after preparation. Something went wrong.\")\n",
    "        \n",
    "    print(f\"Data prepared for training with {len(MODEL_FEATURES)} features.\")\n",
    "    return X, y, X_test, submission_df\n",
    "\n",
    "# Let's run the pipeline.\n",
    "paths = {'eth_train': ETH_TRAIN_PATH, 'eth_test': ETH_TEST_PATH, 'sub': SUB_PATH}\n",
    "X_train, y_train, X_test, submission_df = load_and_prepare_data(paths)\n",
    "\n",
    "# ==============================================================================\n",
    "# ## 4. Model Training & Stacking\n",
    "#\n",
    "# My modeling strategy is a stacking ensemble.\n",
    "# 1.  I use TimeSeriesSplit for cross-validation to make sure I'm not looking into the future.\n",
    "# 2.  I train two strong base models: LightGBM and CatBoost.\n",
    "# 3.  I then train a final, simpler \"meta-model\" on their predictions to learn how to best combine them.\n",
    "# ==============================================================================\n",
    "\n",
    "time_series_splitter = TimeSeriesSplit(n_splits=5)\n",
    "lgbm_oof_predictions = np.zeros(len(X_train))\n",
    "catboost_oof_predictions = np.zeros(len(X_train))\n",
    "\n",
    "print(\"\\n--- Starting Cross-Validation for Base Models ---\")\n",
    "for fold, (train_index, validation_index) in enumerate(time_series_splitter.split(X_train), 1):\n",
    "    print(f\"  Training Fold {fold}...\")\n",
    "    # Splitting the data for this fold.\n",
    "    X_train_fold, X_validation_fold = X_train.iloc[train_index], X_train.iloc[validation_index]\n",
    "    y_train_fold, y_validation_fold = y_train.iloc[train_index], y_train.iloc[validation_index]\n",
    "\n",
    "    # --- Train LightGBM ---\n",
    "    lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "    lgbm.fit(X_train_fold, y_train_fold, eval_set=[(X_validation_fold, y_validation_fold)], callbacks=[lgb.early_stopping(400, verbose=False)])\n",
    "    lgbm_oof_predictions[validation_index] = lgbm.predict(X_validation_fold)\n",
    "\n",
    "    # --- Train CatBoost ---\n",
    "    cat = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "    cat.fit(X_train_fold, y_train_fold, eval_set=[(X_validation_fold, y_validation_fold)])\n",
    "    catboost_oof_predictions[validation_index] = cat.predict(X_validation_fold)\n",
    "\n",
    "# --- Train the Stacking Meta-Model ---\n",
    "print(\"\\n--- Training Stacking Meta-Model ---\")\n",
    "# The features for my meta-model are just the predictions from the base models.\n",
    "meta_features_train = pd.DataFrame({\n",
    "    'lgbm_pred': lgbm_oof_predictions,\n",
    "    'cat_pred': catboost_oof_predictions\n",
    "})\n",
    "# I have to make sure I only train the meta-model on the validation folds.\n",
    "oof_mask = lgbm_oof_predictions != 0\n",
    "meta_model = lgb.LGBMRegressor(**META_MODEL_PARAMS)\n",
    "meta_model.fit(meta_features_train[oof_mask], y_train[oof_mask])\n",
    "\n",
    "# ==============================================================================\n",
    "# ## 5. Evaluation & Final Submission\n",
    "#\n",
    "# Time to see how I did. I'll check the Pearson score for each model and the final\n",
    "# stacked ensemble, then generate the submission file.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Evaluate OOF Predictions ---\n",
    "y_for_oof_eval = y_train[oof_mask]\n",
    "lgbm_oof_eval = lgbm_oof_predictions[oof_mask]\n",
    "catboost_oof_eval = catboost_oof_predictions[oof_mask]\n",
    "stacked_oof_predictions = meta_model.predict(meta_features_train[oof_mask])\n",
    "\n",
    "# Calculate Pearson Correlation for each model.\n",
    "corr_lgbm, _ = pearsonr(y_for_oof_eval, lgbm_oof_eval)\n",
    "corr_cat, _ = pearsonr(y_for_oof_eval, catboost_oof_eval)\n",
    "corr_stacked, _ = pearsonr(y_for_oof_eval, stacked_oof_predictions)\n",
    "\n",
    "print(\"\\n-------------------------------------------\")\n",
    "print(\"  Cross-Validation Performance (Pearson)\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(f\"  LightGBM CV Pearson : {corr_lgbm:.6f}\")\n",
    "print(f\"  CatBoost CV Pearson : {corr_cat:.6f}\")\n",
    "print(f\"  Stacked CV Pearson  : {corr_stacked:.6f} âœ¨\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "# --- Train Final Models on All Data ---\n",
    "print(\"\\nTraining final models on all the data for the submission...\")\n",
    "final_lgbm = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "final_lgbm.fit(X_train, y_train)\n",
    "lgbm_final_predictions = final_lgbm.predict(X_test)\n",
    "\n",
    "final_catboost = CatBoostRegressor(**CATBOOST_PARAMS)\n",
    "final_catboost.fit(X_train, y_train)\n",
    "catboost_final_predictions = final_catboost.predict(X_test)\n",
    "\n",
    "# --- Generate Final Predictions with the Meta-Model ---\n",
    "print(\"Generating final predictions with the trained meta-model...\")\n",
    "meta_features_test = pd.DataFrame({\n",
    "    'lgbm_pred': lgbm_final_predictions,\n",
    "    'cat_pred': catboost_final_predictions\n",
    "})\n",
    "final_predictions = meta_model.predict(meta_features_test)\n",
    "\n",
    "# --- Create and Save Submission File ---\n",
    "submission_df['labels'] = final_predictions\n",
    "try:\n",
    "    submission_df.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"\\nâœ… Stacked ensemble submission successfully saved to: {OUT_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving submission file: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# To improve the model even more, I'd look into adding other data sources, like\n",
    "# the OHLCV data or order book data from other cryptos like BTC.\n",
    "# ==============================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
